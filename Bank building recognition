pip install scikit-optimize
from google.colab import drive
drive.mount('/content/drive')

To check the files(photos) in the folder

import os

directory_path = '/content/drive/MyDrive/TPRPROJECT/PHOTO'

# Check if the directory exists
if os.path.isdir(directory_path):
    # Process the directory
    for file_name in os.listdir(directory_path):
        file_path = os.path.join(directory_path, file_name)
        if os.path.isfile(file_path):
            # Process the file
            print("Processing file:", file_path)
        else:
            # Handle the case if it is not a file
            print("Skipping:", file_path, "is not a file")
else:
    # Handle the case when the directory does not exist
    print("Directory does not exist:", directory_path)
import library needed
import os
import cv2
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report, confusion_matrix, roc_auc_score, log_loss
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import seaborn as sns
Load and preprocess the photos
def preprocess_images(image_folder):
    X = []
    y = []
    
    for file_name in os.listdir(image_folder):
        file_path = os.path.join(image_folder, file_name)
        if os.path.isfile(file_path):
            image = cv2.imread(file_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            image = cv2.resize(image, (64, 64))
            X.append(image.flatten())
            y.append(file_name.split('_')[0])

    return np.array(X), np.array(y)
Using FLD to perform the dimensionality reduction
def perform_fld(X, y, n_components):
    # Initialize the FLD model
    fld = LinearDiscriminantAnalysis(n_components=n_components)
    # Fit the FLD model to X and y
    fld.fit(X, y)
    # Transform X using the fitted FLD model
    X_fld = fld.transform(X)
    return X_fld

To load the image folder
image_folder = '/content/drive/MyDrive/TPRPROJECT/PHOTO' 

To load and prepocess the images into X, y
X, y = preprocess_images(image_folder)

from google.colab import drive
drive.mount('/content/drive')
To find out the original dimension of the data (before dimensionality reduction)
print("Original dimension:", X.shape[1])

Input the desired dimension
n_components = 2  # Specify the desired number of components
X_fld = perform_fld(X, y, n_components)
To check if the dimension is successfully reduced by what we input on the previous line
print("Reduced dimension:", X_fld.shape[1])
Assign colors based on the labels using a colormap
unique_labels = np.unique(y)
colors = plt.cm.get_cmap('tab10', len(unique_labels))
Create a scatter plot of the reduced feature space
for i, label in enumerate(unique_labels):
    class_data = X_fld[y == label]
    plt.scatter(class_data[:, 0], class_data[:, 1], color=colors(i), label=label)

plt.title('FLD Reduced Feature Space')
plt.xlabel('FLD Dimension 1')
plt.ylabel('FLD Dimension 2')
plt.legend()
plt.show()
import os
import cv2
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report, confusion_matrix, roc_auc_score, log_loss
from skopt import BayesSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score
# Data Augmentation
X = X.reshape(-1, 64, 64, 1)
datagen = ImageDataGenerator(
    rotation_range=20,
    zoom_range=0.2,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    vertical_flip=True)
datagen.fit(X)
# Flatten the images back into a 2D array
X = X.reshape(X.shape[0], -1)
n_components = 2
X_fld = perform_fld(X, y, n_components)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_fld, y, test_size=0.2, random_state=42)
param_space = {
    'n_estimators': [100, 150, 200, 250],
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [5, 10, 15],
    'min_samples_leaf': [1, 2, 4, 8],
    'max_features': ['auto', 'sqrt', 'log2']
}
clf = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_distributions=param_space, n_iter=200, cv=5, n_jobs=-1)

# Train the classifier
clf.fit(X_train, y_train)
# Perform cross-validation
cv_scores = cross_val_score(clf, X_train, y_train, cv=5)
print(f'Cross-validation scores: {cv_scores}')
print(f'Average cross-validation score: {cv_scores.mean()}')
# Print the best parameters
print(f'Best parameters: {clf.best_params_}')
# Make predictions on the test set using the best model
y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)
# Calculate the metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
logloss = log_loss(y_test, y_pred_proba)
print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'F1 Score: {f1}')
print(f'ROC AUC Score: {roc_auc}')
print(f'Log Loss: {logloss}')

# Print a classification report
print(classification_report(y_test, y_pred))
# Calculate the confusion matrix
confusion_matrix = confusion_matrix(y_test, y_pred)

# Create a plot and set the figure size
plt.figure(figsize=(10, 7))

# Create a heatmap of the confusion matrix with annotations
sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')

# Add labels to the plot and show it
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
# Assuming y_test and y_pred are your actual and predicted labels
plt.figure(figsize=(10, 6))
plt.scatter(range(len(y_test)), y_test, color='red', label='Actual')
plt.scatter(range(len(y_pred)), y_pred, color='blue', label='Predicted')
plt.title('Test Set Results')
plt.xlabel('Sample Index')
plt.ylabel('Label')
plt.legend()
plt.show()
# Function to preprocess a single image
def preprocess_single_image(image_path):
    # Read the image
    image = cv2.imread(image_path)
    # Convert the image to grayscale
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Resize the image to 64x64
    image = cv2.resize(image, (64, 64))
    # Flatten the image and return it as a 2D array
    X = image.flatten()
    return np.array([X])
# Path to the new image
image_path = '/content/x1.jpg'
# Preprocess the new image
X_new = preprocess_single_image(image_path)

fld = LinearDiscriminantAnalysis(n_components=n_components)

fld.fit(X, y)

X_fld = fld.transform(X)

X_new_fld = fld.transform(X_new)

y_new_pred = clf.predict(X_new_fld)

print(y_new_pred)
y_pred_proba = clf.predict_proba(X_new_fld)
y_pred_proba
